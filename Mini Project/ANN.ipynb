{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN for Increasing samples and Increasing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1,y1 = make_classification(n_samples=1000,n_features=5,n_informative=2,n_redundant=1,random_state=1)\n",
    "X2,y2 = make_classification(n_samples=10000,n_features=5,n_informative=2,n_redundant=1,random_state=1)\n",
    "X3,y3 = make_classification(n_samples=100000,n_features=5,n_informative=2,n_redundant=1,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "400/400 - 1s - loss: 0.5236 - accuracy: 0.7500\n",
      "Epoch 2/10\n",
      "400/400 - 0s - loss: 0.4194 - accuracy: 0.8263\n",
      "Epoch 3/10\n",
      "400/400 - 0s - loss: 0.3687 - accuracy: 0.8413\n",
      "Epoch 4/10\n",
      "400/400 - 0s - loss: 0.3458 - accuracy: 0.8512\n",
      "Epoch 5/10\n",
      "400/400 - 0s - loss: 0.3379 - accuracy: 0.8512\n",
      "Epoch 6/10\n",
      "400/400 - 0s - loss: 0.3352 - accuracy: 0.8537\n",
      "Epoch 7/10\n",
      "400/400 - 0s - loss: 0.3332 - accuracy: 0.8537\n",
      "Epoch 8/10\n",
      "400/400 - 0s - loss: 0.3324 - accuracy: 0.8575\n",
      "Epoch 9/10\n",
      "400/400 - 0s - loss: 0.3314 - accuracy: 0.8600\n",
      "Epoch 10/10\n",
      "400/400 - 0s - loss: 0.3304 - accuracy: 0.8587\n",
      "Dataset 1\n",
      "[[66 25]\n",
      " [16 93]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.73      0.76        91\n",
      "           1       0.79      0.85      0.82       109\n",
      "\n",
      "    accuracy                           0.80       200\n",
      "   macro avg       0.80      0.79      0.79       200\n",
      "weighted avg       0.80      0.80      0.79       200\n",
      "\n",
      "\n",
      "=====================================================================\n",
      "Epoch 1/10\n",
      "4000/4000 - 2s - loss: 0.3420 - accuracy: 0.8518\n",
      "Epoch 2/10\n",
      "4000/4000 - 2s - loss: 0.1817 - accuracy: 0.9450\n",
      "Epoch 3/10\n",
      "4000/4000 - 2s - loss: 0.1593 - accuracy: 0.9557\n",
      "Epoch 4/10\n",
      "4000/4000 - 2s - loss: 0.1517 - accuracy: 0.9571\n",
      "Epoch 5/10\n",
      "4000/4000 - 2s - loss: 0.1484 - accuracy: 0.9578\n",
      "Epoch 6/10\n",
      "4000/4000 - 2s - loss: 0.1462 - accuracy: 0.9580\n",
      "Epoch 7/10\n",
      "4000/4000 - 2s - loss: 0.1446 - accuracy: 0.9601\n",
      "Epoch 8/10\n",
      "4000/4000 - 2s - loss: 0.1448 - accuracy: 0.9588\n",
      "Epoch 9/10\n",
      "4000/4000 - 2s - loss: 0.1437 - accuracy: 0.9592\n",
      "Epoch 10/10\n",
      "4000/4000 - 2s - loss: 0.1432 - accuracy: 0.9582\n",
      "Dataset 2\n",
      "[[977  12]\n",
      " [ 71 940]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96       989\n",
      "           1       0.99      0.93      0.96      1011\n",
      "\n",
      "    accuracy                           0.96      2000\n",
      "   macro avg       0.96      0.96      0.96      2000\n",
      "weighted avg       0.96      0.96      0.96      2000\n",
      "\n",
      "\n",
      "=====================================================================\n",
      "Epoch 1/10\n",
      "40000/40000 - 16s - loss: 0.2016 - accuracy: 0.9239\n",
      "Epoch 2/10\n",
      "40000/40000 - 15s - loss: 0.1601 - accuracy: 0.9455\n",
      "Epoch 3/10\n",
      "40000/40000 - 15s - loss: 0.1538 - accuracy: 0.9489\n",
      "Epoch 4/10\n",
      "40000/40000 - 16s - loss: 0.1508 - accuracy: 0.9497\n",
      "Epoch 5/10\n",
      "40000/40000 - 16s - loss: 0.1485 - accuracy: 0.9499\n",
      "Epoch 6/10\n",
      "40000/40000 - 16s - loss: 0.1452 - accuracy: 0.9505\n",
      "Epoch 7/10\n",
      "40000/40000 - 16s - loss: 0.1416 - accuracy: 0.9506\n",
      "Epoch 8/10\n",
      "40000/40000 - 16s - loss: 0.1394 - accuracy: 0.9514\n",
      "Epoch 9/10\n",
      "40000/40000 - 16s - loss: 0.1384 - accuracy: 0.9522\n",
      "Epoch 10/10\n",
      "40000/40000 - 16s - loss: 0.1384 - accuracy: 0.9519\n",
      "Dataset 3\n",
      "[[9591  385]\n",
      " [ 545 9479]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      9976\n",
      "           1       0.96      0.95      0.95     10024\n",
      "\n",
      "    accuracy                           0.95     20000\n",
      "   macro avg       0.95      0.95      0.95     20000\n",
      "weighted avg       0.95      0.95      0.95     20000\n",
      "\n",
      "\n",
      "=====================================================================\n"
     ]
    }
   ],
   "source": [
    "Datasets=[[X1,y1,\"Dataset 1\"],\n",
    "         [X2,y2,\"Dataset 2\"],\n",
    "         [X3,y3,\"Dataset 3\"]]\n",
    "sc=StandardScaler()\n",
    "for r in Datasets:\n",
    "    Xtrain,Xtest,Ytrain,Ytest=train_test_split(r[0],r[1],test_size=0.2)\n",
    "    Xtrain=sc.fit_transform(Xtrain)\n",
    "    Xtest=sc.transform(Xtest)\n",
    "    ann = Sequential()\n",
    "    ann.add(Dense(units=5,activation='relu'))\n",
    "    ann.add(Dense(units=5,activation='relu'))\n",
    "    ann.add(Dense(1,activation='sigmoid'))\n",
    "    ann.compile(loss='binary_crossentropy',optimizer='ADAM',metrics=['accuracy'])\n",
    "    ann.fit(Xtrain,Ytrain,epochs=10,batch_size=2,verbose=2)\n",
    "    Ypred = ann.predict(Xtest)\n",
    "    YpredF=np.zeros(len(Ytest))\n",
    "    #0.5 Threshold value\n",
    "    for i in range (len(Ytest)):\n",
    "        if(Ypred[i]>=0.5):\n",
    "            YpredF[i]=1\n",
    "        else:\n",
    "            continue\n",
    "    print(r[2])\n",
    "    print(confusion_matrix(Ytest,YpredF))\n",
    "    print(classification_report(Ytest,YpredF))\n",
    "    print(\"\\n=====================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4000/4000 - 2s - loss: 0.3339 - accuracy: 0.8575\n",
      "Epoch 2/10\n",
      "4000/4000 - 2s - loss: 0.2014 - accuracy: 0.9370\n",
      "Epoch 3/10\n",
      "4000/4000 - 2s - loss: 0.1688 - accuracy: 0.9507\n",
      "Epoch 4/10\n",
      "4000/4000 - 2s - loss: 0.1577 - accuracy: 0.9535\n",
      "Epoch 5/10\n",
      "4000/4000 - 2s - loss: 0.1518 - accuracy: 0.9551\n",
      "Epoch 6/10\n",
      "4000/4000 - 2s - loss: 0.1494 - accuracy: 0.9545\n",
      "Epoch 7/10\n",
      "4000/4000 - 2s - loss: 0.1474 - accuracy: 0.9546\n",
      "Epoch 8/10\n",
      "4000/4000 - 2s - loss: 0.1449 - accuracy: 0.9560\n",
      "Epoch 9/10\n",
      "4000/4000 - 2s - loss: 0.1439 - accuracy: 0.9575\n",
      "Epoch 10/10\n",
      "4000/4000 - 2s - loss: 0.1418 - accuracy: 0.9565\n",
      "Dataset 4\n",
      "[[969  32]\n",
      " [ 38 961]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97      1001\n",
      "           1       0.97      0.96      0.96       999\n",
      "\n",
      "    accuracy                           0.96      2000\n",
      "   macro avg       0.97      0.96      0.96      2000\n",
      "weighted avg       0.97      0.96      0.96      2000\n",
      "\n",
      "\n",
      "=====================================================================\n",
      "Epoch 1/10\n",
      "4000/4000 - 2s - loss: 0.3919 - accuracy: 0.8207\n",
      "Epoch 2/10\n",
      "4000/4000 - 2s - loss: 0.2396 - accuracy: 0.9135\n",
      "Epoch 3/10\n",
      "4000/4000 - 2s - loss: 0.1977 - accuracy: 0.9383\n",
      "Epoch 4/10\n",
      "4000/4000 - 2s - loss: 0.1804 - accuracy: 0.9459\n",
      "Epoch 5/10\n",
      "4000/4000 - 2s - loss: 0.1705 - accuracy: 0.9495\n",
      "Epoch 6/10\n",
      "4000/4000 - 2s - loss: 0.1595 - accuracy: 0.9550\n",
      "Epoch 7/10\n",
      "4000/4000 - 2s - loss: 0.1523 - accuracy: 0.9571\n",
      "Epoch 8/10\n",
      "4000/4000 - 2s - loss: 0.1462 - accuracy: 0.9575\n",
      "Epoch 9/10\n",
      "4000/4000 - 2s - loss: 0.1425 - accuracy: 0.9589\n",
      "Epoch 10/10\n",
      "4000/4000 - 2s - loss: 0.1396 - accuracy: 0.9590\n",
      "Dataset 5\n",
      "[[984  27]\n",
      " [ 79 910]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      1011\n",
      "           1       0.97      0.92      0.94       989\n",
      "\n",
      "    accuracy                           0.95      2000\n",
      "   macro avg       0.95      0.95      0.95      2000\n",
      "weighted avg       0.95      0.95      0.95      2000\n",
      "\n",
      "\n",
      "=====================================================================\n",
      "Epoch 1/10\n",
      "4000/4000 - 2s - loss: 0.4314 - accuracy: 0.7995\n",
      "Epoch 2/10\n",
      "4000/4000 - 2s - loss: 0.2802 - accuracy: 0.8924\n",
      "Epoch 3/10\n",
      "4000/4000 - 2s - loss: 0.2258 - accuracy: 0.9233\n",
      "Epoch 4/10\n",
      "4000/4000 - 2s - loss: 0.1978 - accuracy: 0.9385\n",
      "Epoch 5/10\n",
      "4000/4000 - 2s - loss: 0.1811 - accuracy: 0.9427\n",
      "Epoch 6/10\n",
      "4000/4000 - 2s - loss: 0.1657 - accuracy: 0.9479\n",
      "Epoch 7/10\n",
      "4000/4000 - 2s - loss: 0.1520 - accuracy: 0.9534\n",
      "Epoch 8/10\n",
      "4000/4000 - 2s - loss: 0.1427 - accuracy: 0.9536\n",
      "Epoch 9/10\n",
      "4000/4000 - 2s - loss: 0.1353 - accuracy: 0.9567\n",
      "Epoch 10/10\n",
      "4000/4000 - 2s - loss: 0.1287 - accuracy: 0.9588\n",
      "Dataset 6\n",
      "[[972  42]\n",
      " [ 64 922]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      1014\n",
      "           1       0.96      0.94      0.95       986\n",
      "\n",
      "    accuracy                           0.95      2000\n",
      "   macro avg       0.95      0.95      0.95      2000\n",
      "weighted avg       0.95      0.95      0.95      2000\n",
      "\n",
      "\n",
      "=====================================================================\n"
     ]
    }
   ],
   "source": [
    "X4,y4 = make_classification(n_samples=10000,n_features=10,n_informative=2,n_redundant=1,random_state=1)\n",
    "X5,y5 = make_classification(n_samples=10000,n_features=50,n_informative=2,n_redundant=1,random_state=1)\n",
    "X6,y6 = make_classification(n_samples=10000,n_features=100,n_informative=2,n_redundant=1,random_state=1)\n",
    "Datasets2=[[X4,y4,\"Dataset 4\"],\n",
    "         [X5,y5,\"Dataset 5\"],\n",
    "         [X6,y6,\"Dataset 6\"]]\n",
    "for r in Datasets2:\n",
    "    Xtrain,Xtest,Ytrain,Ytest=train_test_split(r[0],r[1],test_size=0.2)\n",
    "    Xtrain=sc.fit_transform(Xtrain)\n",
    "    Xtest=sc.transform(Xtest)\n",
    "    ann = Sequential()\n",
    "    ann.add(Dense(units=10,activation='relu'))\n",
    "    ann.add(Dense(units=10,activation='relu'))\n",
    "    ann.add(Dense(1,activation='sigmoid'))\n",
    "    ann.compile(loss='binary_crossentropy',optimizer='ADAM',metrics=['accuracy'])\n",
    "    ann.fit(Xtrain,Ytrain,epochs=10,batch_size=2,verbose=2)\n",
    "    Ypred = ann.predict(Xtest)\n",
    "    YpredF=np.zeros(len(Ytest))\n",
    "    #0.5 Threshold value\n",
    "    for i in range (len(Ytest)):\n",
    "        if(Ypred[i]>=0.5):\n",
    "            YpredF[i]=1\n",
    "        else:\n",
    "            continue\n",
    "    print(r[2])\n",
    "    print(confusion_matrix(Ytest,YpredF))\n",
    "    print(classification_report(Ytest,YpredF))\n",
    "    print(\"\\n=====================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[943  53]\n",
      " [ 90 914]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93       996\n",
      "           1       0.95      0.91      0.93      1004\n",
      "\n",
      "    accuracy                           0.93      2000\n",
      "   macro avg       0.93      0.93      0.93      2000\n",
      "weighted avg       0.93      0.93      0.93      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X,y = make_classification(n_samples=10000,n_features=100,n_informative=2,n_redundant=1,random_state=1)\n",
    "# sc=StandardScaler()\n",
    "Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,y,test_size=0.2)\n",
    "# Xtrain=sc.fit_transform(Xtrain)\n",
    "# Xtest=sc.transform(Xtest)\n",
    "\n",
    "\n",
    "ann = Sequential()\n",
    "ann.add(Dense(units=5,activation='relu'))\n",
    "ann.add(Dense(units=5,activation='relu'))\n",
    "ann.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "ann.compile(loss='binary_crossentropy',optimizer='ADAM',metrics=['accuracy'])\n",
    "\n",
    "ann.fit(Xtrain,Ytrain,epochs=50,batch_size=2,verbose=0)\n",
    "\n",
    "Ypred = ann.predict(Xtest)\n",
    "YpredF=np.zeros(len(Ytest))\n",
    "#0.5 Threshold value\n",
    "for i in range (len(Ytest)):\n",
    "    if(Ypred[i]>=0.5):\n",
    "        YpredF[i]=1\n",
    "    else:\n",
    "        continue\n",
    "# print(r[2])\n",
    "print(confusion_matrix(Ytest,YpredF))\n",
    "print(classification_report(Ytest,YpredF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "Xscaled = scaler.fit_transform(X)\n",
    "Xtrain,Xtest,Ytrain,Ytest=train_test_split(Xscaled,y,test_size=0.2)\n",
    "pca = PCA(n_components = 0.95)\n",
    "Xtrain = pca.fit_transform(Xtrain)\n",
    "Xtest = pca.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[994  48]\n",
      " [ 52 906]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95      1042\n",
      "           1       0.95      0.95      0.95       958\n",
      "\n",
      "    accuracy                           0.95      2000\n",
      "   macro avg       0.95      0.95      0.95      2000\n",
      "weighted avg       0.95      0.95      0.95      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ann = Sequential()\n",
    "ann.add(Dense(units=5,activation='relu'))\n",
    "ann.add(Dense(units=5,activation='relu'))\n",
    "ann.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "ann.compile(loss='binary_crossentropy',optimizer='ADAM',metrics=['accuracy'])\n",
    "\n",
    "ann.fit(Xtrain,Ytrain,epochs=50,batch_size=2,verbose=0)\n",
    "\n",
    "Ypred = ann.predict(Xtest)\n",
    "YpredF=np.zeros(len(Ytest))\n",
    "#0.5 Threshold value\n",
    "for i in range (len(Ytest)):\n",
    "    if(Ypred[i]>=0.5):\n",
    "        YpredF[i]=1\n",
    "    else:\n",
    "        continue\n",
    "# print(r[2])\n",
    "print(confusion_matrix(Ytest,YpredF))\n",
    "print(classification_report(Ytest,YpredF))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
